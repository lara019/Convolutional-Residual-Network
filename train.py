'''
  VERSION 1.0
'''
from __future__ import print_function
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils
from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping

import numpy as np
import resnet
import time
import datetime

save_dir = ""
version = '1.0'
lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)
early_stopper = EarlyStopping(min_delta=0.001, patience=10)
#csv_logger = CSVLogger(save_dir + nombre + '.csv') -> dentro de las funciones train_DA

batch_size = 32
nb_epoch = 200
verbose=1

def test(model, x_test, y_test, title):
    print('model predict')
    y_pred, x_recon = model.predict(x_test, batch_size=100)
    print('-'*50)
    print('Test acc: ' + title, np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])


def train_sin_DA(model_sin_DA, nombre, data):
 
  print('Training ' + nombre + ' Not using data augmentation ', str(datetime.datetime.now()))

  (x_train, y_train), (x_test, y_test) = data
  csv_logger = CSVLogger(save_dir + nombre + 'sin_DA.csv')

  history_callback = model_sin_DA.fit(x_train, y_train, batch_size=batch_size,
    nb_epoch=nb_epoch,validation_data=(x_test, y_test), verbose = verbose,
    shuffle=True, callbacks=[lr_reducer, early_stopper, csv_logger])

  #test(model_sin_DA, x_test, y_test, nombre + 'sin_DA')  
  save("nombre", model_sin_DA)
  loss_history = history_callback.history["loss"]
  numpy_loss_history = np.array(loss_history)
  np.savetxt(nombre + "loss_history.txt", numpy_loss_history, delimiter=",")

  print('FIN: Training ' + nombre + ' Not using data augmentation ', str(datetime.datetime.now()))



def train_DA(model_DA, nombre, data):
  
  t = datetime.datetime.now()
  print('Training ' + nombre + 'Using real-time data augmentation ' + str(datetime.datetime.now()))
  (x_train, y_train), (x_test, y_test) = data
  # This will do preprocessing and realtime data augmentation:
  datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=False,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=False,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=True,  # randomly flip images
    vertical_flip=False)  # randomly flip images

  # Compute quantities required for featurewise normalization
  # (std, mean, and principal components if ZCA whitening is applied).
  datagen.fit(x_train)
  csv_logger = CSVLogger(save_dir + nombre + '_DA.csv')
  # Fit the model on the batches generated by datagen.flow().
  history_callback =model_DA.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), 
    steps_per_epoch=x_train.shape[0] // batch_size,
    validation_data=(x_test, y_test),
    epochs=nb_epoch, verbose=verbose, max_q_size=100,
    callbacks=[lr_reducer, early_stopper, csv_logger])

  save(nombre, model_DA)
  print('FIN. Trainin ' + nombre + 'Using real-time data augmentation ' + str(datetime.datetime.now()))
  loss_history = history_callback.history["loss"]
  numpy_loss_history = np.array(loss_history)
  np.savetxt(nombre + "loss_history.txt", numpy_loss_history, delimiter=",")

  #test(model_sin_DA, x_test, y_test, nombre + '_DA')  


def save(nombre, model):
  model_yaml = model.to_yaml()
  with open(save_dir + nombre + '.yaml', "w") as yaml_file:
    yaml_file.write(save_dir + model_yaml)
  # serialize weights to HDF5
  model.save_weights(save_dir + nombre + '.h5')
  print('Trained model saved ')


def train(model, nombre, data):
  print('****************************')
  print('****************************')
  print('********', nombre, '  ', version, '***')
  print('****************************')
  print('****************************')
  model.compile(loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy'])

  model.summary()
  model_DA = model

  print("Start time sinDA: ", str(datetime.datetime.now()))
  start_time_sinDA = time.clock()

  train_sin_DA(model, nombre, data)

  start_time_conDA = time.clock()

  train_DA(model_DA, nombre, data)
  fin_time_conDA = time.clock()
  print("Fin time conDA: ", str(datetime.datetime.now()))


  print (start_time_conDA - start_time_sinDA, "seconds sin DA")
  print (fin_time_conDA - start_time_conDA, "seconds con DA")
